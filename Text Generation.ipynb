{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    with open(filepath) as f:\n",
    "        str_text = f.read()\n",
    "    return str_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize and clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm',disable=['parser','tagger','ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length= 1198623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seprate_fun(t):\n",
    "    return [token.text.lower() for token in nlp(t) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = read_file('UPDATED_NLP_COURSE/06-Deep-Learning/moby_dick_four_chapters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = seprate_fun(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11338"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence of token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 25 + 1  # 25 words for training and last word for prediction\n",
    "    \n",
    "text_seq = []\n",
    "\n",
    "for i in range(train_len , len(tokens)):\n",
    "    \n",
    "    seq = tokens[i-train_len : i]\n",
    "    \n",
    "    text_seq.append(seq)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['call',\n",
       " 'me',\n",
       " 'ishmael',\n",
       " 'some',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'never',\n",
       " 'mind',\n",
       " 'how',\n",
       " 'long',\n",
       " 'precisely',\n",
       " 'having',\n",
       " 'little',\n",
       " 'or',\n",
       " 'no',\n",
       " 'money',\n",
       " 'in',\n",
       " 'my',\n",
       " 'purse',\n",
       " 'and',\n",
       " 'nothing',\n",
       " 'particular',\n",
       " 'to',\n",
       " 'interest',\n",
       " 'me',\n",
       " 'on']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_seq[0]  # first sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode token into number of array list\n",
    "tokenizer = Tokenizer() \n",
    "tokenizer.fit_on_texts(text_seq)\n",
    "sequence = tokenizer.texts_to_sequences(text_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[956,\n",
       " 14,\n",
       " 263,\n",
       " 51,\n",
       " 261,\n",
       " 408,\n",
       " 87,\n",
       " 219,\n",
       " 129,\n",
       " 111,\n",
       " 954,\n",
       " 260,\n",
       " 50,\n",
       " 43,\n",
       " 38,\n",
       " 315,\n",
       " 7,\n",
       " 23,\n",
       " 546,\n",
       " 3,\n",
       " 150,\n",
       " 259,\n",
       " 6,\n",
       " 2712,\n",
       " 14,\n",
       " 24]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "956:call\n",
      "14:me\n",
      "263:ishmael\n",
      "51:some\n",
      "261:years\n",
      "408:ago\n",
      "87:never\n",
      "219:mind\n",
      "129:how\n",
      "111:long\n",
      "954:precisely\n",
      "260:having\n",
      "50:little\n",
      "43:or\n",
      "38:no\n",
      "315:money\n",
      "7:in\n",
      "23:my\n",
      "546:purse\n",
      "3:and\n",
      "150:nothing\n",
      "259:particular\n",
      "6:to\n",
      "2712:interest\n",
      "14:me\n",
      "24:on\n"
     ]
    }
   ],
   "source": [
    "for i in sequence[0]:\n",
    "    print(f\"{i}:{tokenizer.index_word[i]}\") # same sequence converted to encode numbers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11312"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_counts # how many times each word occur in given text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2717"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert list to numpy matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_array = np.array(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 956,   14,  263,   51,  261,  408,   87,  219,  129,  111,  954,\n",
       "        260,   50,   43,   38,  315,    7,   23,  546,    3,  150,  259,\n",
       "          6, 2712,   14,   24])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_array[0] # first sequence in nd.array format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = len(sequence_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11312, 26)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , LSTM , Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size , seq_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size ,output_dim = 25 ,input_length = seq_len))\n",
    "    model.add(LSTM(128 , return_sequences = True))\n",
    "    model.add(LSTM(128 , return_sequences = False))\n",
    "    model.add(Dense(256 , activation = 'relu'))\n",
    "    model.add(Dense(vocab_size , activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy' , optimizer='adam' , metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 956,   14,  263, ..., 2712,   14,   24],\n",
       "       [  14,  263,   51, ...,   14,   24,  957],\n",
       "       [ 263,   51,  261, ...,   24,  957,    5],\n",
       "       ...,\n",
       "       [ 952,   12,  166, ...,  262,   53,    2],\n",
       "       [  12,  166, 2711, ...,   53,    2, 2717],\n",
       "       [ 166, 2711,    3, ...,    2, 2717,   26]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 956,   14,  263, ...,    6, 2712,   14],\n",
       "       [  14,  263,   51, ..., 2712,   14,   24],\n",
       "       [ 263,   51,  261, ...,   14,   24,  957],\n",
       "       ...,\n",
       "       [ 952,   12,  166, ...,   11,  262,   53],\n",
       "       [  12,  166, 2711, ...,  262,   53,    2],\n",
       "       [ 166, 2711,    3, ...,   53,    2, 2717]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_array[:, :-1] # except last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  24],\n",
       "       [ 957],\n",
       "       [   5],\n",
       "       ...,\n",
       "       [   2],\n",
       "       [2717],\n",
       "       [  26]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_array[: , -1:] # last column which we have to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sequence_array[: , :-1]\n",
    "y = sequence_array[: , -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y , num_classes=vocab_size + 1) # we add 1 because for unknown word which index is zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 25, 25)            67950     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 25, 128)           78848     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2718)              698526    \n",
      "=================================================================\n",
      "Total params: 1,009,932\n",
      "Trainable params: 1,009,932\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocab_size + 1 , seq_len=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "11312/11312 [==============================] - 19s 2ms/step - loss: 6.8332 - accuracy: 0.0523\n",
      "Epoch 2/250\n",
      "11312/11312 [==============================] - 18s 2ms/step - loss: 6.3839 - accuracy: 0.0508\n",
      "Epoch 3/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 6.3364 - accuracy: 0.0529\n",
      "Epoch 4/250\n",
      "11312/11312 [==============================] - 17s 2ms/step - loss: 6.1977 - accuracy: 0.0529\n",
      "Epoch 5/250\n",
      "11312/11312 [==============================] - 17s 2ms/step - loss: 6.1226 - accuracy: 0.0529\n",
      "Epoch 6/250\n",
      "11312/11312 [==============================] - 18s 2ms/step - loss: 6.0721 - accuracy: 0.0554\n",
      "Epoch 7/250\n",
      "11312/11312 [==============================] - 18s 2ms/step - loss: 6.0023 - accuracy: 0.0627\n",
      "Epoch 8/250\n",
      "11312/11312 [==============================] - 18s 2ms/step - loss: 5.8735 - accuracy: 0.0659 2s - loss: 5.8\n",
      "Epoch 9/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 5.7691 - accuracy: 0.0700\n",
      "Epoch 10/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 5.6796 - accuracy: 0.0729\n",
      "Epoch 11/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 5.5966 - accuracy: 0.0730\n",
      "Epoch 12/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 5.5250 - accuracy: 0.0754\n",
      "Epoch 13/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 5.4522 - accuracy: 0.0774\n",
      "Epoch 14/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 5.3828 - accuracy: 0.0797\n",
      "Epoch 15/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 5.3165 - accuracy: 0.0804\n",
      "Epoch 16/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 5.2480 - accuracy: 0.0853\n",
      "Epoch 17/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 5.2156 - accuracy: 0.0878\n",
      "Epoch 18/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 5.1308 - accuracy: 0.0867 2s - l\n",
      "Epoch 19/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 5.0692 - accuracy: 0.0887\n",
      "Epoch 20/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 5.0111 - accuracy: 0.0905\n",
      "Epoch 21/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 4.9512 - accuracy: 0.0907\n",
      "Epoch 22/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 4.8968 - accuracy: 0.0917\n",
      "Epoch 23/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 4.8371 - accuracy: 0.0949\n",
      "Epoch 24/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 4.7912 - accuracy: 0.0945\n",
      "Epoch 25/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 4.7405 - accuracy: 0.0978\n",
      "Epoch 26/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 4.6935 - accuracy: 0.0987\n",
      "Epoch 27/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 4.6457 - accuracy: 0.1010\n",
      "Epoch 28/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 4.5930 - accuracy: 0.1023\n",
      "Epoch 29/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 4.5449 - accuracy: 0.1048\n",
      "Epoch 30/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 4.4980 - accuracy: 0.1069\n",
      "Epoch 31/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 4.4551 - accuracy: 0.1042\n",
      "Epoch 32/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 4.4075 - accuracy: 0.1069\n",
      "Epoch 33/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 4.3615 - accuracy: 0.1120 1s - loss: 4.3513 \n",
      "Epoch 34/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 4.3139 - accuracy: 0.1121 0s - loss: 4.3120 - accuracy: 0.\n",
      "Epoch 35/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 4.2710 - accuracy: 0.1152\n",
      "Epoch 36/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 4.2197 - accuracy: 0.1154\n",
      "Epoch 37/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 4.1823 - accuracy: 0.1223\n",
      "Epoch 38/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 4.1317 - accuracy: 0.1230\n",
      "Epoch 39/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 4.0878 - accuracy: 0.1255\n",
      "Epoch 40/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 4.0483 - accuracy: 0.1267\n",
      "Epoch 41/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 3.9973 - accuracy: 0.1324\n",
      "Epoch 42/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 3.9541 - accuracy: 0.1321\n",
      "Epoch 43/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 3.9162 - accuracy: 0.1342\n",
      "Epoch 44/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 3.8708 - accuracy: 0.1382\n",
      "Epoch 45/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 3.8292 - accuracy: 0.1423 1s - loss: 3.8205 - \n",
      "Epoch 46/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 3.7821 - accuracy: 0.1481\n",
      "Epoch 47/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 3.7410 - accuracy: 0.1533\n",
      "Epoch 48/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 3.7037 - accuracy: 0.1588\n",
      "Epoch 49/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 3.6617 - accuracy: 0.1641\n",
      "Epoch 50/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 3.6260 - accuracy: 0.1685\n",
      "Epoch 51/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 3.5862 - accuracy: 0.1734\n",
      "Epoch 52/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 3.5454 - accuracy: 0.1772 0s - loss: 3.5449 - accuracy: 0.\n",
      "Epoch 53/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 3.5103 - accuracy: 0.1826\n",
      "Epoch 54/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 3.4736 - accuracy: 0.1838\n",
      "Epoch 55/250\n",
      "11312/11312 [==============================] - 15s 1ms/step - loss: 3.4297 - accuracy: 0.1910\n",
      "Epoch 56/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 3.4013 - accuracy: 0.1980\n",
      "Epoch 57/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 3.3609 - accuracy: 0.2030\n",
      "Epoch 58/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 3.3222 - accuracy: 0.2087\n",
      "Epoch 59/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 3.2896 - accuracy: 0.2139\n",
      "Epoch 60/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 3.2655 - accuracy: 0.2181\n",
      "Epoch 61/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 3.2269 - accuracy: 0.2216\n",
      "Epoch 62/250\n",
      "11312/11312 [==============================] - 15s 1ms/step - loss: 3.2050 - accuracy: 0.2257\n",
      "Epoch 63/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 3.1621 - accuracy: 0.2344 2s\n",
      "Epoch 64/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 3.1224 - accuracy: 0.2363\n",
      "Epoch 65/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 3.0933 - accuracy: 0.2432\n",
      "Epoch 66/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 3.0576 - accuracy: 0.2525\n",
      "Epoch 67/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 3.0232 - accuracy: 0.2579\n",
      "Epoch 68/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.9928 - accuracy: 0.2671\n",
      "Epoch 69/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.9646 - accuracy: 0.2739\n",
      "Epoch 70/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.9345 - accuracy: 0.2809\n",
      "Epoch 71/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.9036 - accuracy: 0.2777\n",
      "Epoch 72/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.8847 - accuracy: 0.2833\n",
      "Epoch 73/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.8489 - accuracy: 0.2930\n",
      "Epoch 74/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 2.8195 - accuracy: 0.2976\n",
      "Epoch 75/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.7915 - accuracy: 0.3014\n",
      "Epoch 76/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.7544 - accuracy: 0.3124\n",
      "Epoch 77/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.7284 - accuracy: 0.3150\n",
      "Epoch 78/250\n",
      "11312/11312 [==============================] - 15s 1ms/step - loss: 2.7050 - accuracy: 0.3256\n",
      "Epoch 79/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.6781 - accuracy: 0.3304\n",
      "Epoch 80/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.6472 - accuracy: 0.3342\n",
      "Epoch 81/250\n",
      "11312/11312 [==============================] - 15s 1ms/step - loss: 2.6158 - accuracy: 0.3375\n",
      "Epoch 82/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.5981 - accuracy: 0.3407\n",
      "Epoch 83/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.5787 - accuracy: 0.3485\n",
      "Epoch 84/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.5571 - accuracy: 0.3528\n",
      "Epoch 85/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.5121 - accuracy: 0.3633 0s - loss: 2.5123 - accuracy: 0.36\n",
      "Epoch 86/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.4906 - accuracy: 0.3678\n",
      "Epoch 87/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.4648 - accuracy: 0.3710\n",
      "Epoch 88/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.4399 - accuracy: 0.3804\n",
      "Epoch 89/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.4134 - accuracy: 0.3899 0s - loss: 2.4133 - accuracy: 0.39\n",
      "Epoch 90/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 2.3875 - accuracy: 0.3935\n",
      "Epoch 91/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.3725 - accuracy: 0.3951\n",
      "Epoch 92/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.3694 - accuracy: 0.3938\n",
      "Epoch 93/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.3299 - accuracy: 0.4059\n",
      "Epoch 94/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.2987 - accuracy: 0.4135 1s - loss: 2.2885 - \n",
      "Epoch 95/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.2772 - accuracy: 0.4205 0s - loss: 2.2761 - accuracy: 0.42\n",
      "Epoch 96/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.2611 - accuracy: 0.4215\n",
      "Epoch 97/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.2416 - accuracy: 0.4258\n",
      "Epoch 98/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.2132 - accuracy: 0.4297\n",
      "Epoch 99/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.1915 - accuracy: 0.4379\n",
      "Epoch 100/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.1714 - accuracy: 0.4415\n",
      "Epoch 101/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.1527 - accuracy: 0.4419\n",
      "Epoch 102/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.1371 - accuracy: 0.4483\n",
      "Epoch 103/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.1047 - accuracy: 0.4585\n",
      "Epoch 104/250\n",
      "11312/11312 [==============================] - 15s 1ms/step - loss: 2.0815 - accuracy: 0.4602\n",
      "Epoch 105/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.0940 - accuracy: 0.4560\n",
      "Epoch 106/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.0690 - accuracy: 0.4609\n",
      "Epoch 107/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.0291 - accuracy: 0.4716\n",
      "Epoch 108/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 2.0153 - accuracy: 0.4790\n",
      "Epoch 109/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.9980 - accuracy: 0.4838\n",
      "Epoch 110/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.9770 - accuracy: 0.4879\n",
      "Epoch 111/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.9650 - accuracy: 0.4913\n",
      "Epoch 112/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.9417 - accuracy: 0.4951\n",
      "Epoch 113/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.9200 - accuracy: 0.4989\n",
      "Epoch 114/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.8979 - accuracy: 0.5053\n",
      "Epoch 115/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.8866 - accuracy: 0.5069\n",
      "Epoch 116/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.8746 - accuracy: 0.5118\n",
      "Epoch 117/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.8503 - accuracy: 0.5211\n",
      "Epoch 118/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.8308 - accuracy: 0.5213\n",
      "Epoch 119/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.8280 - accuracy: 0.5221\n",
      "Epoch 120/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.8071 - accuracy: 0.5265\n",
      "Epoch 121/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.7826 - accuracy: 0.5339\n",
      "Epoch 122/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.7699 - accuracy: 0.5374\n",
      "Epoch 123/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.7510 - accuracy: 0.5396 5s - loss: 1.7209 - accu\n",
      "Epoch 124/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 1.7295 - accuracy: 0.5494\n",
      "Epoch 125/250\n",
      "11312/11312 [==============================] - 17s 2ms/step - loss: 1.7266 - accuracy: 0.5468\n",
      "Epoch 126/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.7012 - accuracy: 0.5553 0s - loss: 1.6989 - accuracy\n",
      "Epoch 127/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.6921 - accuracy: 0.5575\n",
      "Epoch 128/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.6789 - accuracy: 0.5609\n",
      "Epoch 129/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.6523 - accuracy: 0.5626\n",
      "Epoch 130/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.6370 - accuracy: 0.5709\n",
      "Epoch 131/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.6285 - accuracy: 0.5720\n",
      "Epoch 132/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.6136 - accuracy: 0.5748\n",
      "Epoch 133/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.6176 - accuracy: 0.5738\n",
      "Epoch 134/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.5898 - accuracy: 0.5836\n",
      "Epoch 135/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.5831 - accuracy: 0.5804\n",
      "Epoch 136/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.5724 - accuracy: 0.5817\n",
      "Epoch 137/250\n",
      "11312/11312 [==============================] - 15s 1ms/step - loss: 1.5508 - accuracy: 0.5920\n",
      "Epoch 138/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.5454 - accuracy: 0.5853\n",
      "Epoch 139/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.5115 - accuracy: 0.6001\n",
      "Epoch 140/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.4976 - accuracy: 0.6059\n",
      "Epoch 141/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.4832 - accuracy: 0.6060\n",
      "Epoch 142/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.4634 - accuracy: 0.6118\n",
      "Epoch 143/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.4453 - accuracy: 0.6180\n",
      "Epoch 144/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.4466 - accuracy: 0.6171\n",
      "Epoch 145/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.4362 - accuracy: 0.6164\n",
      "Epoch 146/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.4305 - accuracy: 0.6192\n",
      "Epoch 147/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.4085 - accuracy: 0.6236\n",
      "Epoch 148/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.3830 - accuracy: 0.6337\n",
      "Epoch 149/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.3713 - accuracy: 0.6382\n",
      "Epoch 150/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.3623 - accuracy: 0.6362\n",
      "Epoch 151/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.3486 - accuracy: 0.6414\n",
      "Epoch 152/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.3376 - accuracy: 0.6414\n",
      "Epoch 153/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.3210 - accuracy: 0.6508\n",
      "Epoch 154/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.3088 - accuracy: 0.6514\n",
      "Epoch 155/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.2896 - accuracy: 0.6578\n",
      "Epoch 156/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.2792 - accuracy: 0.6612\n",
      "Epoch 157/250\n",
      "11312/11312 [==============================] - 17s 2ms/step - loss: 1.2674 - accuracy: 0.6634\n",
      "Epoch 158/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 1.2723 - accuracy: 0.6565\n",
      "Epoch 159/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.2563 - accuracy: 0.6641\n",
      "Epoch 160/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.2390 - accuracy: 0.6699\n",
      "Epoch 161/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.2277 - accuracy: 0.6682\n",
      "Epoch 162/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.2430 - accuracy: 0.6669\n",
      "Epoch 163/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.2207 - accuracy: 0.6680\n",
      "Epoch 164/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.1823 - accuracy: 0.6869\n",
      "Epoch 165/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.1687 - accuracy: 0.6855 4s - loss: 1.1428 - accuracy: \n",
      "Epoch 166/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.1525 - accuracy: 0.6955 3s - - ETA: 0s - loss: 1.1490 - accuracy\n",
      "Epoch 167/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.1389 - accuracy: 0.6978\n",
      "Epoch 168/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.1407 - accuracy: 0.6941\n",
      "Epoch 169/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 1.1259 - accuracy: 0.6975\n",
      "Epoch 170/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.1121 - accuracy: 0.7016\n",
      "Epoch 171/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.1036 - accuracy: 0.7062\n",
      "Epoch 172/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.0887 - accuracy: 0.7077\n",
      "Epoch 173/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.0856 - accuracy: 0.7104\n",
      "Epoch 174/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.0795 - accuracy: 0.7093\n",
      "Epoch 175/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.0658 - accuracy: 0.7159\n",
      "Epoch 176/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 1.0492 - accuracy: 0.7206 0s - loss: 1.0476 - accuracy: 0.\n",
      "Epoch 177/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.0410 - accuracy: 0.7204 2s - loss: 1\n",
      "Epoch 178/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.0279 - accuracy: 0.7262\n",
      "Epoch 179/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.0233 - accuracy: 0.7238\n",
      "Epoch 180/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.0114 - accuracy: 0.7300\n",
      "Epoch 181/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.9912 - accuracy: 0.7361\n",
      "Epoch 182/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.9846 - accuracy: 0.7397\n",
      "Epoch 183/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.9789 - accuracy: 0.7384\n",
      "Epoch 184/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.9596 - accuracy: 0.7436\n",
      "Epoch 185/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.9461 - accuracy: 0.7527\n",
      "Epoch 186/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.9349 - accuracy: 0.7508 2s - loss: 0\n",
      "Epoch 187/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.9176 - accuracy: 0.7574 1s - loss: 0.908\n",
      "Epoch 188/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.9032 - accuracy: 0.7593\n",
      "Epoch 189/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.8911 - accuracy: 0.7664 2s -\n",
      "Epoch 190/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.8854 - accuracy: 0.7694\n",
      "Epoch 191/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 0.8818 - accuracy: 0.7702\n",
      "Epoch 192/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 0.8704 - accuracy: 0.7731\n",
      "Epoch 193/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.8720 - accuracy: 0.7695\n",
      "Epoch 194/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.8567 - accuracy: 0.7717\n",
      "Epoch 195/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.8373 - accuracy: 0.7831\n",
      "Epoch 196/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.8252 - accuracy: 0.7824\n",
      "Epoch 197/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 1.0900 - accuracy: 0.7489\n",
      "Epoch 198/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.9947 - accuracy: 0.7544\n",
      "Epoch 199/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 0.9219 - accuracy: 0.7635\n",
      "Epoch 200/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 0.8776 - accuracy: 0.7757 2s - loss: 0\n",
      "Epoch 201/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 0.8247 - accuracy: 0.7845\n",
      "Epoch 202/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 0.8275 - accuracy: 0.7825\n",
      "Epoch 203/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.8125 - accuracy: 0.7894\n",
      "Epoch 204/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.7950 - accuracy: 0.7885\n",
      "Epoch 205/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.7640 - accuracy: 0.8040\n",
      "Epoch 206/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 0.7344 - accuracy: 0.8083\n",
      "Epoch 207/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.7263 - accuracy: 0.8148\n",
      "Epoch 208/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.7322 - accuracy: 0.8137\n",
      "Epoch 209/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.7311 - accuracy: 0.8083\n",
      "Epoch 210/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 0.7256 - accuracy: 0.8070\n",
      "Epoch 211/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 0.7273 - accuracy: 0.8170\n",
      "Epoch 212/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 0.8276 - accuracy: 0.7983 2s - los\n",
      "Epoch 213/250\n",
      "11312/11312 [==============================] - 17s 2ms/step - loss: 0.7241 - accuracy: 0.8206\n",
      "Epoch 214/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.6982 - accuracy: 0.8185\n",
      "Epoch 215/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.6864 - accuracy: 0.8240\n",
      "Epoch 216/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.6786 - accuracy: 0.8267\n",
      "Epoch 217/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.6614 - accuracy: 0.8309\n",
      "Epoch 218/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.6289 - accuracy: 0.8445\n",
      "Epoch 219/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.6161 - accuracy: 0.8475\n",
      "Epoch 220/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.6125 - accuracy: 0.8449\n",
      "Epoch 221/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 0.6049 - accuracy: 0.8515\n",
      "Epoch 222/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 0.9557 - accuracy: 0.7918\n",
      "Epoch 223/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 1.0028 - accuracy: 0.7686\n",
      "Epoch 224/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11312/11312 [==============================] - 17s 1ms/step - loss: 0.9456 - accuracy: 0.7821\n",
      "Epoch 225/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 0.8247 - accuracy: 0.8032\n",
      "Epoch 226/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.7706 - accuracy: 0.8128\n",
      "Epoch 227/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 0.7380 - accuracy: 0.8227\n",
      "Epoch 228/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 0.7036 - accuracy: 0.8305\n",
      "Epoch 229/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.6875 - accuracy: 0.8340\n",
      "Epoch 230/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.6675 - accuracy: 0.8365 0s - loss: 0.6665 - accuracy: 0.\n",
      "Epoch 231/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 0.6511 - accuracy: 0.8386\n",
      "Epoch 232/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 0.6536 - accuracy: 0.8371\n",
      "Epoch 233/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 0.6314 - accuracy: 0.8456\n",
      "Epoch 234/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 0.6437 - accuracy: 0.8420\n",
      "Epoch 235/250\n",
      "11312/11312 [==============================] - 17s 2ms/step - loss: 0.6450 - accuracy: 0.8393\n",
      "Epoch 236/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.6025 - accuracy: 0.8464 1s - loss: 0.6006 - accuracy: 0. - ETA: 0s - loss: 0.6005 - accu\n",
      "Epoch 237/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.8167 - accuracy: 0.8141\n",
      "Epoch 238/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 0.6834 - accuracy: 0.8206\n",
      "Epoch 239/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 0.6405 - accuracy: 0.8274\n",
      "Epoch 240/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.5891 - accuracy: 0.8396\n",
      "Epoch 241/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.5047 - accuracy: 0.8751\n",
      "Epoch 242/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 0.4576 - accuracy: 0.8892\n",
      "Epoch 243/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 0.4310 - accuracy: 0.9016\n",
      "Epoch 244/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 0.4108 - accuracy: 0.9090\n",
      "Epoch 245/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 0.4031 - accuracy: 0.9098\n",
      "Epoch 246/250\n",
      "11312/11312 [==============================] - 17s 2ms/step - loss: 0.3915 - accuracy: 0.9143 3s\n",
      "Epoch 247/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.3891 - accuracy: 0.9137\n",
      "Epoch 248/250\n",
      "11312/11312 [==============================] - 16s 1ms/step - loss: 0.3853 - accuracy: 0.9153\n",
      "Epoch 249/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 0.3794 - accuracy: 0.9166 2s -\n",
      "Epoch 250/250\n",
      "11312/11312 [==============================] - 17s 1ms/step - loss: 0.3949 - accuracy: 0.9108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x22230db2dc8>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X , y ,batch_size= 128,epochs = 250,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump,load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genrating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genrate_text(tokenizer,seed_text , seq_len , model , num_gen_words):\n",
    "    \n",
    "    output_text = []\n",
    "    input_text = seed_text\n",
    "    \n",
    "    for i in range(num_gen_words):\n",
    "        \n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        \n",
    "        pad_encoded = pad_sequences([encoded_text] , maxlen=seq_len , truncating='pre')\n",
    "        \n",
    "        pred_word_ind = model.predict_classes(pad_encoded , verbose = 0)[0]\n",
    "        \n",
    "        pred_word = tokenizer.index_word[pred_word_ind]\n",
    "        \n",
    "        input_text += ' '+pred_word\n",
    "        \n",
    "        output_text.append(pred_word)\n",
    "        \n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11312"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed()\n",
    "random_pick = random.randint(0 , len(text_seq))\n",
    "seed_text = ' '.join(text_seq[random_pick])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"again and seeing no possible chance of spending a sufferable night unless in some other person 's bed i began to think that after all i\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'might be cherishing unwarrantable prejudices against this unknown harpooneer thinks i the bar room when knowing all it was a very dubious looking nay a very dark'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genrate_text(tokenizer=tokenizer , seed_text=seed_text , seq_len = seq_len , model=model ,num_gen_words=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
